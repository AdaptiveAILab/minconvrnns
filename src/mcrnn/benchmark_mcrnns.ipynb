{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 3601,
     "status": "ok",
     "timestamp": 1748339045958,
     "user": {
      "displayName": "Matthias Karlbauer",
      "userId": "03030495491604295596"
     },
     "user_tz": -120
    },
    "id": "XP3I5jCDVIhH",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# if not torch.cuda.is_available():\n",
    "#     import torch_xla.core.xla_model as xm  # For use of a TPU (CUDATimer class can't be used on TPUs)\n",
    "#     import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA H100 80GB HBM3\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 48,
     "status": "ok",
     "timestamp": 1748339177259,
     "user": {
      "displayName": "Matthias Karlbauer",
      "userId": "03030495491604295596"
     },
     "user_tz": -120
    },
    "id": "vjPccZUSVTRx",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CUDATimer:\n",
    "\n",
    "    def __init__(self):\n",
    "        self._starter = torch.cuda.Event(enable_timing=True)\n",
    "        self._ender = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "    def reset(self):\n",
    "        self._starter.record()\n",
    "\n",
    "    def time(self):\n",
    "        self._ender.record()\n",
    "        torch.cuda.synchronize()\n",
    "        forward_time = self._starter.elapsed_time(self._ender)\n",
    "        return forward_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1748339045997,
     "user": {
      "displayName": "Matthias Karlbauer",
      "userId": "03030495491604295596"
     },
     "user_tz": -120
    },
    "id": "7rQ2wtX7-g9e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ConvLSTMCell(nn.Module):\n",
    "    \"\"\"\n",
    "    A ConvLSTM implementation using Conv2d operations.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        batch_size,\n",
    "        input_size,\n",
    "        hidden_size,\n",
    "        height,\n",
    "        width,\n",
    "        device,\n",
    "        bias=True,\n",
    "        padding_mode: str = \"zeros\",\n",
    "        **kwargs\n",
    "    ):\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "\n",
    "        # Parameters\n",
    "        self.batch_size = batch_size\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.bias = bias\n",
    "        self.device = device\n",
    "\n",
    "\n",
    "        # Hidden (h) and cell (c) states\n",
    "        self.h = torch.zeros(size=(batch_size, hidden_size, height, width), device=device)\n",
    "        self.c = torch.zeros(size=(batch_size, hidden_size, height, width), device=device)\n",
    "\n",
    "        # Convolution weights\n",
    "        conv = []\n",
    "        conv.append(nn.Conv2d(\n",
    "            in_channels=input_size + hidden_size,\n",
    "            out_channels=hidden_size*4,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1,\n",
    "            padding_mode=padding_mode,\n",
    "            bias=bias\n",
    "        ))\n",
    "        self.conv = nn.Sequential(*conv)\n",
    "\n",
    "    def reset_states(self, batch_size, height: int = 64, width: int = 64):\n",
    "        if self.batch_size == batch_size and self.height == height and self.width == width:\n",
    "            self.h = torch.zeros_like(self.h)\n",
    "            self.c = torch.zeros_like(self.c)\n",
    "        else:\n",
    "            self.batch_size = batch_size\n",
    "            self.height = height\n",
    "            self.width = width\n",
    "            self.h = torch.zeros(size=(batch_size, self.hidden_size, height, width), device=self.device)\n",
    "            self.c = torch.zeros(size=(batch_size, self.hidden_size, height, width), device=self.device)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # Uniform distribution initialization of lstm weights with respect to\n",
    "        # the number of lstm cells in the layer\n",
    "        std = 1.0/np.sqrt(self.hidden_size)\n",
    "        for w in self.parameters():\n",
    "            w.data.uniform_(-std, std)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        h_prev: torch.Tensor = None,\n",
    "        c_prev: torch.Tensor = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        ...\n",
    "        \"\"\"\n",
    "\n",
    "        # Set the previous hidden and cell states if not provided\n",
    "        h_prev = self.h if h_prev is None else h_prev\n",
    "        c_prev = self.c if c_prev is None else c_prev\n",
    "\n",
    "        # Perform input and recurrent convolutions\n",
    "        conv_res = self.conv(torch.cat((x, h_prev), dim=1))\n",
    "\n",
    "        # Split result into input and gate activations\n",
    "        netin, igate, fgate, ogate = torch.split(conv_res, self.hidden_size, dim=1)\n",
    "\n",
    "        # Compute input and gate activations\n",
    "        act_input = torch.tanh(netin)\n",
    "        act_igate = torch.sigmoid(igate)\n",
    "        act_fgate = torch.sigmoid(fgate)\n",
    "        act_ogate = torch.sigmoid(ogate)\n",
    "\n",
    "        # Compute the new cell and hidden states\n",
    "        c_curr = act_fgate*c_prev + act_igate*act_input\n",
    "        h_curr = act_ogate*torch.tanh(c_curr)\n",
    "\n",
    "        # Update the hidden and cells states\n",
    "        self.h = h_curr\n",
    "        self.c = c_curr\n",
    "\n",
    "        return h_curr, c_curr\n",
    "\n",
    "\n",
    "class ConvLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    A ConvLSTM implementation using Conv1d instead of Conv2d operations.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        batch_size: int = 8,\n",
    "        input_size: int = 1,\n",
    "        hidden_sizes: List = [4, 4],\n",
    "        height: int = 16,\n",
    "        width: int = 16,\n",
    "        device: torch.device = torch.device(\"cpu\"),\n",
    "        bias: bool = True,\n",
    "        padding_mode: str = \"zeros\",\n",
    "        tanh_encoder: bool = False,\n",
    "        norm = nn.LayerNorm,  # Must be overridden with a OmegaConfig\n",
    "        **kwargs\n",
    "    ):\n",
    "        super(ConvLSTM, self).__init__()\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        self.encoder = []\n",
    "        self.encoder.append(torch.nn.Conv2d(\n",
    "            in_channels=input_size,\n",
    "            out_channels=hidden_sizes[0],\n",
    "            kernel_size=1  # When using padding, pass the padding_mode here and use CylinderPad with geopotential\n",
    "        ))\n",
    "        if tanh_encoder: self.encoder.append(torch.nn.Tanh())\n",
    "        self.encoder = torch.nn.Sequential(*self.encoder)\n",
    "\n",
    "        self.clstm = torch.nn.ModuleList()\n",
    "        self.norms = torch.nn.ModuleList()\n",
    "        for h in hidden_sizes:\n",
    "            self.clstm.append(ConvLSTMCell(\n",
    "                batch_size=batch_size,\n",
    "                input_size=h,\n",
    "                hidden_size=h,\n",
    "                height=height,\n",
    "                width=width,\n",
    "                device=device,\n",
    "                bias=bias,\n",
    "                padding_mode=padding_mode\n",
    "            ))\n",
    "            self.norms.append(norm(normalized_shape=(height, width)))\n",
    "        self.clstm = torch.nn.ModuleList(self.clstm)\n",
    "\n",
    "        self.decoder = torch.nn.Conv2d(\n",
    "            in_channels=hidden_sizes[-1],\n",
    "            out_channels=input_size,\n",
    "            kernel_size=1\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        tf_steps: int = 10,  # teacher forcing steps\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        ...\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize hidden and cell states of LSTM to zero\n",
    "        b, t, c, h, w = x.shape\n",
    "        self.reset(batch_size=b, height=h, width=w)\n",
    "        outs = []\n",
    "\n",
    "        # Iterate over sequence\n",
    "        for t in range(x.shape[1]):\n",
    "            # During teacher forcing, take the ground truth as input. In closed loop, take the last model output.\n",
    "            x_t = x[:, t] if t < tf_steps else x_t\n",
    "            # Forward the current time step's input through the model\n",
    "            x_t = self.encoder(x_t)  # [b, hidden, h, w]\n",
    "            for clstm_cell, norm in zip(self.clstm, self.norms):\n",
    "                z_t = x_t\n",
    "                z_t, _ = clstm_cell(z_t)  # [b, hidden, h, w]\n",
    "                x_t = x_t + z_t  # residual connection\n",
    "                x_t = norm(x_t)\n",
    "            x_t = self.decoder(x_t)\n",
    "            outs.append(x_t)\n",
    "\n",
    "        return torch.stack(outs, dim=1)\n",
    "\n",
    "    def reset(self, batch_size: int = 8, height: int = 64, width: int = 64):\n",
    "        for clstm_cell in self.clstm:\n",
    "            clstm_cell.reset_states(batch_size=batch_size, height=height, width=width)\n",
    "        self.zeros = torch.zeros(size=(batch_size, 1, height, width), device=self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 67,
     "status": "ok",
     "timestamp": 1748339046050,
     "user": {
      "displayName": "Matthias Karlbauer",
      "userId": "03030495491604295596"
     },
     "user_tz": -120
    },
    "id": "oZXVlzmRsWBu",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class minConvLSTMCell(nn.Module):\n",
    "    \"\"\"\n",
    "    A ConvLSTM implementation using Conv2d operations.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        batch_size: int = 1,\n",
    "        input_size: int = 1,\n",
    "        hidden_size: int = 16,\n",
    "        height: int = 16,\n",
    "        width: int = 16,\n",
    "        device: torch.device = torch.device(\"cpu\"),\n",
    "        bias: bool = True,\n",
    "        padding_mode: str = \"zeros\",\n",
    "        exponentiate: bool = False,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super(minConvLSTMCell, self).__init__()\n",
    "\n",
    "        # Parameters\n",
    "        self.batch_size = batch_size\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.bias = bias\n",
    "        self.device = device\n",
    "        self.exponentiate = exponentiate\n",
    "\n",
    "        self.h = torch.zeros(size=(batch_size, hidden_size, height, width), device=device)\n",
    "\n",
    "        # Convolution weights\n",
    "        conv = []\n",
    "        conv.append(nn.Conv2d(\n",
    "            in_channels=hidden_size,\n",
    "            out_channels=hidden_size*3,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1,\n",
    "            padding_mode=padding_mode,\n",
    "            bias=bias\n",
    "        ))\n",
    "        self.conv = nn.Sequential(*conv)\n",
    "\n",
    "    def reset_states(self, batch_size, height: int = 16, width: int = 16):\n",
    "        if self.batch_size == batch_size and self.height == height and self.width == width:\n",
    "            self.h = torch.zeros_like(self.h)\n",
    "        else:\n",
    "            self.batch_size = batch_size\n",
    "            self.height = height\n",
    "            self.width = width\n",
    "            self.h = torch.zeros(size=(batch_size, self.hidden_size, height, width), device=self.device)\n",
    "\n",
    "    @staticmethod\n",
    "    def g(x):\n",
    "        return torch.where(x >= 0, x + 0.5, torch.sigmoid(x))\n",
    "\n",
    "    @staticmethod\n",
    "    def log_g(x):\n",
    "        return torch.where(x >= 0, (F.relu(x) + 0.5).log(), -F.softplus(-x))\n",
    "\n",
    "    @staticmethod\n",
    "    def parallel_scan_log(log_coeffs: torch.Tensor, log_values: torch.Tensor) -> torch.Tensor:\n",
    "        # log_coeffs: [B, T, hid, h, w]\n",
    "        # log_values: [B, T+1, hid, h, w]\n",
    "\n",
    "        a_star = F.pad(torch.cumsum(log_coeffs, dim=1), (0, 0, 0, 0, 0, 0, 1, 0))\n",
    "        log_h0_plus_b_star = torch.logcumsumexp(log_values - a_star, dim=1)\n",
    "        log_h = a_star + log_h0_plus_b_star\n",
    "\n",
    "        return torch.exp(log_h)[:, 1:].contiguous()  # [B, T, hid, h, w]\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        ...\n",
    "        \"\"\"\n",
    "\n",
    "        bt, c, h, w = x.shape[0], x.shape[1], x.shape[2], x.shape[3]\n",
    "        b = self.batch_size\n",
    "        t = int(bt/b)\n",
    "\n",
    "        h0 = self.h.unsqueeze(1)\n",
    "\n",
    "        f_gate, i_gate, h_tilde = torch.chunk(\n",
    "            self.conv(x).view(b, t, self.hidden_size*3, h, w).contiguous(),\n",
    "            chunks=3,\n",
    "            dim=2\n",
    "        )\n",
    "\n",
    "        diff = i_gate - f_gate if self.exponentiate else F.softplus(-f_gate) - F.softplus(-i_gate)\n",
    "        log_f = -F.softplus(diff)\n",
    "        log_i = -F.softplus(-diff)\n",
    "        log_h0 = self.log_g(h0)\n",
    "        log_h_tilde = self.log_g(h_tilde)\n",
    "        out = self.parallel_scan_log(log_f, torch.cat([log_h0, log_i + log_h_tilde], dim=1))\n",
    "        self.h = out[:, -1]\n",
    "        out = out.view(bt, self.hidden_size, h, w)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def step(self, x_t: torch.Tensor, h_prev: torch.Tensor = None) -> torch.Tensor:\n",
    "        # sequential mode of minLSTM trained in log-space\n",
    "        # x_t:\n",
    "        # h_prev:\n",
    "\n",
    "        h_prev = self.h if h_prev is None else h_prev\n",
    "\n",
    "        # I get nan exactly in this block until h_curr\n",
    "        f_t, i_t, h_tilde_t = torch.chunk(self.conv(x_t), chunks=3, dim=1)\n",
    "\n",
    "        if self.exponentiate:\n",
    "            f_t, i_t = torch.exp(f_t), torch.exp(i_t)\n",
    "        else:\n",
    "            f_t, i_t = torch.sigmoid(f_t), torch.sigmoid(i_t)\n",
    "        h_tilde_t = self.g(h_tilde_t)\n",
    "        f_prime_t = f_t / (f_t + i_t)\n",
    "        i_prime_t = i_t / (f_t + i_t)\n",
    "        h_curr = f_prime_t * h_prev + i_prime_t * h_tilde_t\n",
    "\n",
    "        self.h = h_curr\n",
    "\n",
    "        return h_curr\n",
    "\n",
    "\n",
    "class minConvLSTM(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        batch_size: int = 8,\n",
    "        input_size: int = 1,\n",
    "        hidden_sizes: List = [4, 4],\n",
    "        height: int = 16,\n",
    "        width: int = 16,\n",
    "        device: torch.device = torch.device(\"cpu\"),\n",
    "        bias: bool = True,\n",
    "        padding_mode: str = \"zeros\",\n",
    "        tanh_encoder: bool = False,\n",
    "        exponentiate: bool = False,\n",
    "        norm = nn.LayerNorm,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super(minConvLSTM, self).__init__()\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        self.encoder = []\n",
    "        self.encoder.append(torch.nn.Conv2d(\n",
    "            in_channels=input_size,\n",
    "            out_channels=hidden_sizes[0],\n",
    "            kernel_size=1,  # When using padding, pass the padding_mode here and use CylinderPad with geopotential\n",
    "        ))\n",
    "        if tanh_encoder: self.encoder.append(torch.nn.Tanh())\n",
    "        self.encoder = torch.nn.Sequential(*self.encoder)\n",
    "\n",
    "        self.clstm = torch.nn.ModuleList()\n",
    "        self.norms = torch.nn.ModuleList()\n",
    "        for h in hidden_sizes:\n",
    "            self.clstm.append(minConvLSTMCell(\n",
    "                batch_size=batch_size,\n",
    "                input_size=h,\n",
    "                hidden_size=h,\n",
    "                height=height,\n",
    "                width=width,\n",
    "                device=device,\n",
    "                bias=bias,\n",
    "                padding_mode=padding_mode,\n",
    "                exponentiate=exponentiate\n",
    "            ))\n",
    "            self.norms.append(norm(normalized_shape=(height, width)))\n",
    "\n",
    "        self.decoder = torch.nn.Conv2d(\n",
    "            in_channels=hidden_sizes[-1],\n",
    "            out_channels=input_size,\n",
    "            kernel_size=1\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, tf_steps: int = 500, test: bool = True) -> torch.Tensor:\n",
    "        b, t, c, h, w = x.shape\n",
    "        tf_steps = min(tf_steps, t)\n",
    "        self.reset(batch_size=b, height=h, width=w)\n",
    "\n",
    "        x_tf = x[:, :tf_steps].reshape(b*tf_steps, c, h, w)\n",
    "        outs = []\n",
    "\n",
    "        #\n",
    "        # Parallel mode for teacher forcing\n",
    "        x_tf = self.encoder(x_tf)\n",
    "        for clstm_cell, norm in zip(self.clstm, self.norms):\n",
    "            # apply skip connection + post layer group norm\n",
    "            z = x_tf\n",
    "            z = clstm_cell(z)\n",
    "            x_tf = z + x_tf\n",
    "            x_tf = norm(x_tf)\n",
    "        out = self.decoder(x_tf).view(b, tf_steps, c, h, w)\n",
    "        outs.append(out)\n",
    "\n",
    "        #\n",
    "        # Sequential mode for closed loop prediction\n",
    "        x_t = out[:, -1] # no need for .clone() because changing x_t does not change out -- tested in notebook\n",
    "        # Iterate over sequence\n",
    "        for t in range(t-tf_steps):\n",
    "            # Forward the current time step's input through the model\n",
    "            x_t = self.encoder(x_t)  # [B, C, H, W] C -> num_channels (hidden state)\n",
    "            for clstm_cell, norm in zip(self.clstm, self.norms):\n",
    "                z_t = x_t\n",
    "                z_t = clstm_cell.step(z_t)  # [B, C, H, W]\n",
    "                x_t = x_t + z_t  # residual connection\n",
    "                x_t = norm(x_t)\n",
    "            x_t = self.decoder(x_t) # [B, 1, I, H, W]\n",
    "            outs.append(x_t.unsqueeze(1))\n",
    "        outs = torch.cat(outs, dim=1)\n",
    "\n",
    "        return outs\n",
    "\n",
    "    def reset(self, batch_size: int = 8, height: int = 16, width: int = 16):\n",
    "        for clstm_cell in self.clstm:\n",
    "            clstm_cell.reset_states(batch_size=batch_size, height=height, width=width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1748339046061,
     "user": {
      "displayName": "Matthias Karlbauer",
      "userId": "03030495491604295596"
     },
     "user_tz": -120
    },
    "id": "oOVwe3YofSFt",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sequential(x, n_reps, model):\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        timer = CUDATimer()\n",
    "        timer.reset()\n",
    "    else:\n",
    "        a = time.time()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(n_reps):\n",
    "            model(x)\n",
    "            if not torch.cuda.is_available(): xm.mark_step()  # Crucial for synchronizing TPU execution\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"Sequential:\", timer.time(), \"ms\")\n",
    "    else:\n",
    "        print(\"Sequential:\", time.time()-a, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1748339046065,
     "user": {
      "displayName": "Matthias Karlbauer",
      "userId": "03030495491604295596"
     },
     "user_tz": -120
    },
    "id": "l5-2NDeKfq7F",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parallel(x, n_reps, model):\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        timer = CUDATimer()\n",
    "        timer.reset()\n",
    "    else:\n",
    "        a = time.time()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        #b, t, c, h, w = x.shape\n",
    "        #data = x.view(b*t, c, h, w)\n",
    "        #for i in range(warmup):\n",
    "        #    model(data)\n",
    "        #\n",
    "        #timer.reset()\n",
    "        #\n",
    "        for i in range(n_reps):\n",
    "            model(x)  # convolution on full input\n",
    "            if not torch.cuda.is_available(): xm.mark_step()  # Crucial for synchronizing TPU execution\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"Parallel:  \", timer.time(), \"ms\")\n",
    "    else:\n",
    "        print(\"Parallel:  \", time.time()-a, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1748339046068,
     "user": {
      "displayName": "Matthias Karlbauer",
      "userId": "03030495491604295596"
     },
     "user_tz": -120
    },
    "id": "x2cXcSgPhUbB",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def benchmark(shape, n_reps):\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = xm.xla_device()\n",
    "    #device = torch.device(\"cpu\")\n",
    "    B, T, C, H, W = shape\n",
    "    print(\"\\nBenchmarking shape\", shape, \"on device\", device)\n",
    "\n",
    "    x = torch.randn(B, T, C, H, W).to(device=device)\n",
    "    clstm = ConvLSTM(\n",
    "        batch_size=B,\n",
    "        input_size=C,\n",
    "        hidden_sizes=[12, 12, 12, 12, 12, 12, 12, 12, 12, 12],\n",
    "        height=H,\n",
    "        width=W,\n",
    "        device=device\n",
    "    ).to(device=device)\n",
    "\n",
    "    mclstm = minConvLSTM(\n",
    "        batch_size=B,\n",
    "        input_size=C,\n",
    "        hidden_sizes=[20, 20, 20, 20, 20, 20, 20, 20, 20, 20],\n",
    "        height=H,\n",
    "        width=W,\n",
    "        device=device\n",
    "    ).to(device=device)\n",
    "    \n",
    "    sequential(x=x, n_reps=n_reps, model=clstm)\n",
    "    parallel(x=x, n_reps=n_reps, model=mclstm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7036,
     "status": "ok",
     "timestamp": 1748339137339,
     "user": {
      "displayName": "Matthias Karlbauer",
      "userId": "03030495491604295596"
     },
     "user_tz": -120
    },
    "id": "b9W3-XjrVVbn",
    "outputId": "af4b8fcc-42e4-49cc-ad81-84c3af037295",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Benchmarking shape (4, 10, 1, 4, 4) on device cuda\n",
      "Sequential: 103.60486602783203 ms\n",
      "Parallel:   28.965824127197266 ms\n",
      "\n",
      "Benchmarking shape (4, 10, 1, 4, 4) on device cuda\n",
      "Sequential: 103.73737335205078 ms\n",
      "Parallel:   28.57004737854004 ms\n",
      "\n",
      "Benchmarking shape (4, 10, 1, 8, 8) on device cuda\n",
      "Sequential: 104.56626892089844 ms\n",
      "Parallel:   96.80924987792969 ms\n",
      "\n",
      "Benchmarking shape (4, 10, 1, 16, 16) on device cuda\n",
      "Sequential: 102.74240112304688 ms\n",
      "Parallel:   35.618560791015625 ms\n",
      "\n",
      "Benchmarking shape (4, 10, 1, 32, 32) on device cuda\n",
      "Sequential: 103.731201171875 ms\n",
      "Parallel:   28.887008666992188 ms\n",
      "\n",
      "Benchmarking shape (4, 10, 1, 64, 64) on device cuda\n",
      "Sequential: 138.51504516601562 ms\n",
      "Parallel:   52.65385437011719 ms\n",
      "\n",
      "Benchmarking shape (4, 10, 1, 128, 128) on device cuda\n",
      "Sequential: 136.5806427001953 ms\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 19.88 MiB is free. Including non-PyTorch memory, this process has 79.07 GiB memory in use. Of the allocated memory 78.15 GiB is allocated by PyTorch, and 320.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[16], line 31\u001B[0m\n\u001B[1;32m     28\u001B[0m benchmark(shape\u001B[38;5;241m=\u001B[39m(B, T, C, H, W), n_reps\u001B[38;5;241m=\u001B[39mn_reps)\n\u001B[1;32m     30\u001B[0m B, T, C, H, W \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m4\u001B[39m, seqlen, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m128\u001B[39m, \u001B[38;5;241m128\u001B[39m\n\u001B[0;32m---> 31\u001B[0m \u001B[43mbenchmark\u001B[49m\u001B[43m(\u001B[49m\u001B[43mshape\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mB\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mT\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mC\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mH\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mW\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_reps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_reps\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     33\u001B[0m B, T, C, H, W \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m4\u001B[39m, seqlen, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m256\u001B[39m, \u001B[38;5;241m256\u001B[39m\n\u001B[1;32m     34\u001B[0m benchmark(shape\u001B[38;5;241m=\u001B[39m(B, T, C, H, W), n_reps\u001B[38;5;241m=\u001B[39mn_reps)\n",
      "Cell \u001B[0;32mIn[15], line 30\u001B[0m, in \u001B[0;36mbenchmark\u001B[0;34m(shape, n_reps)\u001B[0m\n\u001B[1;32m     20\u001B[0m mclstm \u001B[38;5;241m=\u001B[39m minConvLSTM(\n\u001B[1;32m     21\u001B[0m     batch_size\u001B[38;5;241m=\u001B[39mB,\n\u001B[1;32m     22\u001B[0m     input_size\u001B[38;5;241m=\u001B[39mC,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     26\u001B[0m     device\u001B[38;5;241m=\u001B[39mdevice\n\u001B[1;32m     27\u001B[0m )\u001B[38;5;241m.\u001B[39mto(device\u001B[38;5;241m=\u001B[39mdevice)\n\u001B[1;32m     29\u001B[0m sequential(x\u001B[38;5;241m=\u001B[39mx, n_reps\u001B[38;5;241m=\u001B[39mn_reps, model\u001B[38;5;241m=\u001B[39mclstm)\n\u001B[0;32m---> 30\u001B[0m \u001B[43mparallel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_reps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_reps\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmclstm\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[7], line 18\u001B[0m, in \u001B[0;36mparallel\u001B[0;34m(x, n_reps, model)\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m th\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m     10\u001B[0m     \u001B[38;5;66;03m#b, t, c, h, w = x.shape\u001B[39;00m\n\u001B[1;32m     11\u001B[0m     \u001B[38;5;66;03m#data = x.view(b*t, c, h, w)\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     15\u001B[0m     \u001B[38;5;66;03m#timer.reset()\u001B[39;00m\n\u001B[1;32m     16\u001B[0m     \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[1;32m     17\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(n_reps):\n\u001B[0;32m---> 18\u001B[0m         \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# convolution on full input\u001B[39;00m\n\u001B[1;32m     19\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m th\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mis_available(): xm\u001B[38;5;241m.\u001B[39mmark_step()  \u001B[38;5;66;03m# Crucial for synchronizing TPU execution\u001B[39;00m\n\u001B[1;32m     21\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m th\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mis_available():\n",
      "File \u001B[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1751\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1749\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1751\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1762\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1757\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1758\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1759\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1760\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1761\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1762\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1764\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1765\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "Cell \u001B[0;32mIn[5], line 195\u001B[0m, in \u001B[0;36mminConvLSTM.forward\u001B[0;34m(self, x, tf_steps, test)\u001B[0m\n\u001B[1;32m    192\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m clstm_cell, norm \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclstm, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorms):\n\u001B[1;32m    193\u001B[0m     \u001B[38;5;66;03m# apply skip connection + post layer group norm\u001B[39;00m\n\u001B[1;32m    194\u001B[0m     z \u001B[38;5;241m=\u001B[39m x_tf\n\u001B[0;32m--> 195\u001B[0m     z \u001B[38;5;241m=\u001B[39m \u001B[43mclstm_cell\u001B[49m\u001B[43m(\u001B[49m\u001B[43mz\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    196\u001B[0m     x_tf \u001B[38;5;241m=\u001B[39m z \u001B[38;5;241m+\u001B[39m x_tf\n\u001B[1;32m    197\u001B[0m     x_tf \u001B[38;5;241m=\u001B[39m norm(x_tf)\n",
      "File \u001B[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1751\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1749\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1751\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1762\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1757\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1758\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1759\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1760\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1761\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1762\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1764\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1765\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "Cell \u001B[0;32mIn[5], line 99\u001B[0m, in \u001B[0;36mminConvLSTMCell.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     97\u001B[0m log_h0 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlog_g(h0)\n\u001B[1;32m     98\u001B[0m log_h_tilde \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlog_g(h_tilde)\n\u001B[0;32m---> 99\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparallel_scan_log\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlog_f\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mth\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcat\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43mlog_h0\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlog_i\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mlog_h_tilde\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdim\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    100\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mh \u001B[38;5;241m=\u001B[39m out[:, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\n\u001B[1;32m    101\u001B[0m out \u001B[38;5;241m=\u001B[39m out\u001B[38;5;241m.\u001B[39mview(bt, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhidden_size, h, w)\n",
      "Cell \u001B[0;32mIn[5], line 72\u001B[0m, in \u001B[0;36mminConvLSTMCell.parallel_scan_log\u001B[0;34m(log_coeffs, log_values)\u001B[0m\n\u001B[1;32m     69\u001B[0m log_h0_plus_b_star \u001B[38;5;241m=\u001B[39m th\u001B[38;5;241m.\u001B[39mlogcumsumexp(log_values \u001B[38;5;241m-\u001B[39m a_star, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m     70\u001B[0m log_h \u001B[38;5;241m=\u001B[39m a_star \u001B[38;5;241m+\u001B[39m log_h0_plus_b_star\n\u001B[0;32m---> 72\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mth\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexp\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlog_h\u001B[49m\u001B[43m)\u001B[49m[:, \u001B[38;5;241m1\u001B[39m:]\u001B[38;5;241m.\u001B[39mcontiguous()\n",
      "\u001B[0;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 19.88 MiB is free. Including non-PyTorch memory, this process has 79.07 GiB memory in use. Of the allocated memory 78.15 GiB is allocated by PyTorch, and 320.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cudnn.enabled = True\n",
    "\n",
    "n_reps = 10\n",
    "\n",
    "seqlen = 10\n",
    "\n",
    "B, T, C, H, W = 4, seqlen, 1, 4, 4\n",
    "benchmark(shape=(B, T, C, H, W), n_reps=n_reps)\n",
    "\n",
    "B, T, C, H, W = 4, seqlen, 1, 4, 4\n",
    "benchmark(shape=(B, T, C, H, W), n_reps=n_reps)\n",
    "\n",
    "#B, T, C, H, W = 4, seqlen, 1, 8, 32\n",
    "#benchmark(shape=(B, T, C, H, W), n_reps=n_reps)\n",
    "\n",
    "B, T, C, H, W = 4, seqlen, 1, 8, 8\n",
    "benchmark(shape=(B, T, C, H, W), n_reps=n_reps)\n",
    "\n",
    "B, T, C, H, W = 4, seqlen, 1, 16, 16\n",
    "benchmark(shape=(B, T, C, H, W), n_reps=n_reps)\n",
    "\n",
    "B, T, C, H, W = 4, seqlen, 1, 32, 32\n",
    "benchmark(shape=(B, T, C, H, W), n_reps=n_reps)\n",
    "\n",
    "B, T, C, H, W = 4, seqlen, 1, 64, 64\n",
    "benchmark(shape=(B, T, C, H, W), n_reps=n_reps)\n",
    "\n",
    "B, T, C, H, W = 4, seqlen, 1, 128, 128\n",
    "benchmark(shape=(B, T, C, H, W), n_reps=n_reps)\n",
    "\n",
    "B, T, C, H, W = 4, seqlen, 1, 256, 256\n",
    "benchmark(shape=(B, T, C, H, W), n_reps=n_reps)\n",
    "\n",
    "B, T, C, H, W = 4, seqlen, 1, 512, 512\n",
    "benchmark(shape=(B, T, C, H, W), n_reps=n_reps)\n",
    "\n",
    "B, T, C, H, W = 4, seqlen, 1, 4, 4\n",
    "benchmark(shape=(B, T, C, H, W), n_reps=n_reps)\n",
    "\n",
    "B, T, C, H, W = 4, seqlen, 1, 32, 32\n",
    "benchmark(shape=(B, T, C, H, W), n_reps=n_reps)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "/v2/external/notebooks/intro.ipynb",
     "timestamp": 1742827687787
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3.9 via SLURM 250422170413",
   "language": "python",
   "name": "jupyter-eg-kernel-slurm-py39-1ipf0ee10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
